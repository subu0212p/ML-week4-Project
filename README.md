# Next Word Predictor using GPT-2

This project fine-tunes a GPT-2 transformer model using Hugging Face's `transformers` library on a custom dataset derived from a biography of Mahatma Gandhi.

### Features:
- Tokenization using `GPT2Tokenizer`
- Fine-tuning with `Trainer` API
- Next-word text generation from prompts
- Evaluation using Perplexity (~41.59)

All code is written in Python and follows best practices for tokenizer alignment and language modeling.
